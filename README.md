Linear and Logistic Regression. Gradient Descent.

Task Description: Detecting Memristor Faults Using Linear Regression

* Model 1: Zero-Intercept Linear Regression
* Model 2: Linear Regression with Intercept

Parameter Interpretation and Fault Classification

Logistic Regression
* Data Preparation and Design Matrix Creation
* Data Splitting
* Model Training and Evaluation
* Plotting Decision Boundaries

Gradient Descent Optimization of the Ackley Function
* Implement Gradient Descent with Decaying Learning Rate
* Implement the Ackley Function
* Calculate and Implement Partial Derivatives
* Initialize Starting Points
* Choose Hyperparameters and Minimize the Ackley Function
* Plot Cost Over Iterations

 PCA and Classification
* Use the Multilayer Perceptron (MLP) from scikit-learn to classify images of hands into 10 different classes. The images are 64x64 pixels, resulting in a large input dimension (4096). To reduce model size and training time, we will use Principal Component Analysis (PCA) for dimensionality reduction.

Neural Networks From Scratch
Objective: Implement a neural network from scratch to solve the same classification task using an automatic differentiation framework.

Classification Algorithms Evaluation
Evaluation of the performance of three common classification algorithms: k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), and Random Forests. Testing these classifiers on three simple toy datasets for binary classification, each exhibiting different characteristics: Dataset 1 has an outlier, Dataset 2 is highly nonlinear but noise-free, and Dataset 3 has a large overlap between the two classes.

K-means and Expectation-Maximization Algorithms
implement the K-means and Expectation-Maximization (EM) algorithms and evaluate them using the Mickey Mouse dataset. We will compare the performance of these algorithms and provide insights into their behaviors and results.

